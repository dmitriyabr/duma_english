# Скорость прироста мастерства узлов в языковых графах знаний (GSE и аналоги)

Универсального ответа на вопрос «сколько правильных ответов подряд нужно для 100 баллов» не существует — скорость зависит от модели, параметров сглаживания и контекста. Ниже — систематизация исследований и практических подходов.

## Что говорит лингвистика: сколько «встреч» нужно со словом

Исследования в области инцидентального и целенаправленного усвоения лексики дают достаточно широкий диапазон:

- **3–5 повторений** — достаточно для начального распознавания формы слова (Pellicer-Sánchez, 2016; eye-tracking исследование)[^32].
- **6–10 встреч** — «есть шанс распознать значение нового слова позже» (Laufer & Nation, 2012)[^29].
- **10–12 повторений** — «разумная цель для большинства слов» (Nation, 2013; Webb, 2007)[^32].
- **14 встреч для рецептивного** и **18+ для продуктивного** знания (Teng, 2016)[^18].
- **8–12 осмысленных встреч** для стабильной ментальной репрезентации, **20–30 распределённых** — для долгосрочного удержания[^20].
- Эффект частоты встреч перестаёт значимо расти после **~20 повторений**[^29].

Важно: частота встреч объясняет лишь ~11% дисперсии в усвоении слов — остальное определяется глубиной обработки, контекстом, типом активности и индивидуальными особенностями[^29].

## Модели мастерства в адаптивных системах

### Bayesian Knowledge Tracing (BKT)

BKT — классическая модель, используемая в интеллектуальных обучающих системах. Скрытая переменная «навык выучен» (бинарная: learned/unlearned) обновляется по формуле Байеса после каждого ответа[^17].

- **Стандартный порог мастерства: 0.95** — то есть модель должна оценить вероятность правильного ответа ≥ 95%[^17][^19].
- Некоторые системы используют более консервативные пороги: **0.98** или **0.99**[^19].
- Система umimecesky.cz использует 4 уровня прогресс-бара: **0.5 → 0.8 → 0.95 → 0.98**[^17].

Скорость достижения порога 0.95 сильно зависит от параметров BKT (P_init, P_learn, P_guess, P_slip). В экспериментах Peláneka оптимизированные пороги колебались от **0.90 до 0.97**[^17].

### Экспоненциальное скользящее среднее (EMA) — метод Peláneka

Pelánек (UMAP 2017) показал, что простой метод EMA работает не хуже сложных моделей для детекции мастерства[^17]:

\[
\theta_k = \alpha \cdot \theta_{k-1} + (1 - \alpha) \cdot c_k
\]

где \( c_k \in \{0, 1\} \) — правильность k-го ответа, \( \alpha \) — параметр сглаживания, мастерство объявляется при \( \theta_k \geq T \).

Минимальное число **подряд правильных ответов** для достижения порога \( T = 0.95 \) при разных \( \alpha \):

| Параметр α | Правильных подряд для T=0.95 |
|-----------|------------------------------|
| 0.70 | 9 |
| 0.75 | 11 |
| 0.80 | **14** |
| 0.85 | 19 |
| 0.90 | 29 |
| 0.95 | 59 |

Формула: \( N \geq \log_\alpha(1 - T) \)[^17]. Это **достаточное, но не необходимое** условие — если были ошибки ранее, нужно больше правильных ответов, но не обязательно подряд.

Ключевой вывод Peláneka: **выбор порогов и входных данных важнее, чем выбор модели**. Простое EMA с правильно настроенными α и T даёт результаты, сопоставимые с BKT[^17].

### Half-Life Regression (HLR) — модель Duolingo

Duolingo использует HLR, где сила слова в памяти моделируется через «период полураспада»[^16]:

\[
\hat{h}_\Theta = 2^{\Theta \cdot \mathbf{x}}, \quad \hat{p}_\Theta = 2^{-\Delta / \hat{h}_\Theta}
\]

- \( \hat{p} \) — вероятность вспомнить слово
- \( \Delta \) — время с последней практики
- \( \mathbf{x} \) — вектор признаков: \( \sqrt{x_\oplus} \) (корень из числа правильных), \( \sqrt{x_\ominus} \) (корень из ошибок), лексемные теги

Навык «золотой» (4 полоски, аналог 100%) = средняя вероятность вспомнить случайное слово из навыка близка к 1.0. **Нет фиксированного числа правильных ответов** — прогресс зависит от времени, истории ошибок и сложности слова[^16]. HLR снизила ошибку предсказания на 45%+ по сравнению с системой Лейтнера и повысила ежедневное вовлечение на 12%[^16].

### SuperMemo SM-2 и Anki

В SM-2 карточка «растёт» по интервалам: 1 день → 6 дней → I(n) = I(n-1) × EF (EF по умолчанию 2.5). Оценка < 3 из 5 **сбрасывает прогресс** на начало[^33]. Anki использует шаги обучения (например, 15 мин → 1 день → 3 дня), после чего карточка «выпускается» в долгосрочную ротацию[^40].

### Система Лейтнера

Карточки перемещаются между «коробками» с интервалами 1, 2, 4, 8, 16 дней. Правильный ответ — повышение, неправильный — понижение. Для достижения максимальной коробки нужно **5 правильных ответов подряд** с увеличивающимися интервалами[^16].

## N подряд правильных (Consecutive Correct) — простейший метод

Метод «N правильных подряд» (NCC) до сих пор широко используется и удивительно эффективен[^17]:

- **Для BKT-сценариев** оптимальные N: **2–8**
- **Для логистических моделей** (медленное обучение): **8–17**
- Общий оптимум: обычно **3–5 для лёгких элементов**, **8–15 для сложных**

Корреляция решений NCC и BKT в экспериментах была **очень высокой (0.74–0.99)** — оба метода объявляют мастерство почти одновременно[^17].

## Практические рекомендации для EdTech-приложения

На основе исследований, оптимальная стратегия для вашего приложения (граф знаний типа GSE):

### Рекомендуемая модель прироста баллов

1. **Используйте EMA с α ≈ 0.8 и T = 0.95** — это хороший баланс между строгостью и мотивацией. При α = 0.8 нужно **~14 правильных подряд** для 100%, но ошибки не обнуляют прогресс полностью — они просто снижают скользящее среднее[^17].

2. **Визуализируйте промежуточные уровни**: 0.5 (50%), 0.8 (80%), 0.95 (95%), как делает umimecesky.cz — это поддерживает мотивацию при виде постепенного прогресса[^17].

3. **Учитывайте время между ответами**: если ученик правильно использует слово через день, неделю и месяц — это надёжнее, чем 10 раз подряд за одну сессию. Распределённая практика даёт 85% удержания через год против 22% при «зубрёжке»[^7].

4. **Не присваивайте 100% слишком быстро**: исследования показывают, что для продуктивного знания слова нужно **минимум 10–12 встреч**[^32], а для долгосрочного удержания — **20–30 распределённых**[^20]. Быстрое присвоение 100% после 3 правильных ответов создаёт иллюзию знания.

5. **Добавьте «распад» мастерства**: как в HLR Duolingo, сила слова должна снижаться со временем, если ученик не практикуется. Это критически важно для лексики, в отличие от процедурных навыков[^16][^17].

### Ориентировочная шкала для вашего приложения

| Контекст использования | Баллы | Комментарий |
|---|---|---|
| 1 правильный ответ | 15–25 | Начальное распознавание |
| 3 правильных подряд | 40–50 | Формирующееся знание |
| 5 правильных в разных контекстах | 60–70 | Уверенное знание |
| 8–10 правильных с интервалами | 80–90 | Устойчивое знание |
| 12–15 распределённых правильных | 95–100 | Мастерство |
| Без практики 2+ недели | −10–20 пп | Распад (decay) |

Эта шкала приблизительно соответствует EMA с α ≈ 0.8 и множественным порогам, и согласуется с данными о 10–12 встречах для устойчивого усвоения[^32][^17].

## Ключевые источники

- **Pelánек & Říhák (2017)** — "Experimental Analysis of Mastery Learning Criteria" (UMAP 2017) — наиболее релевантное исследование, сравнивающее BKT, EMA и NCC для детекции мастерства[^17].
- **Settles & Meeder (2016)** — "A Trainable Spaced Repetition Model for Language Learning" (ACL 2016) — модель HLR Duolingo[^16].
- **Nation (2013)** — "How vocabulary is learned" — количество встреч для усвоения лексики[^32].
- **Webb (2007), Teng (2016)** — исследования частоты экспозиций для рецептивного и продуктивного знания[^18].
- **Uchihara, Webb & Yanagisawa (2019)** — мета-анализ эффекта повторений на инцидентальное усвоение[^29].


---

## References

7. [How to Use Spaced Repetition for Learning New Language](https://www.relearnify.com/posts/how-to-use-spaced-repetition-for-learning-language) - Master spaced repetition for language learning: retain 85% of vocabulary vs 22% with cramming. Prove...

16. [How we learn how you learn](https://blog.duolingo.com/how-we-learn-how-you-learn/) - Experimental results. We conducted several experiments to verify that our new half-life regression m...

17. [[PDF] Experimental Analysis of Mastery Learning Criteria - FI MUNI](https://www.fi.muni.cz/~xpelanek/publications/mastery-detection.pdf) - For difficult knowledge components, the score levels off only after the threshold is over 0.95. The ...

18. [[PDF] Effects of Exposure Frequency, Depth of Processing, and Activity ...](https://files.eric.ed.gov/fulltext/EJ1414704.pdf) - The Effects of Exposure Frequency on Vocabulary Learning. This study shows that exposure frequency p...

19. [[PDF] A Principled Intelligent Occupational Training of Psychomotor Skills ...](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2023/EECS-2023-17.pdf) - We propose to use bayesian knowledge tracing (BKT), a de facto standard in education to predict stud...

20. [What Really Matters in Vocabulary Acquisition? A Ranked Analysis ...](https://gianfrancoconti.com/2025/04/26/what-really-matters-in-vocabulary-acquisition-a-ranked-analysis-of-key-influencing-factors/) - Learners generally need at least 8–12 meaningful encounters to establish a stable mental representat...

29. [How often do you need to encounter a word before you learn it?](https://adaptivelearninginelt.wordpress.com/2019/09/23/how-often-do-you-need-to-encounter-a-word-before-you-learn-it/) - The number of encounters necessary to learn words rang[es] from 6, 10, 12, to more than 20 times. Th...

32. [[PDF] How vocabulary is learned - Paul Nation - Neliti](https://media.neliti.com/media/publications/242518-how-vocabulary-is-learned-45a2c109.pdf) - Abstract. Vocabulary learning requires two basic conditions – repetition. (quantity of meetings with...

33. [java - Spaced repetition algorithm from SuperMemo (SM-2)](https://stackoverflow.com/questions/49047159/spaced-repetition-algorithm-from-supermemo-sm-2) - I want to implement the SuperMemo (SM-2) algorithm in Java. This is a popular choice for spaced repe...

40. [Anki Settings: A Complete Guide and Recommended Settings ...](https://zhighley.com/article/anki-settings/) - How do we “graduate” cards? Well if we go to the settings of Steps and Graduating Interval, once we ...

